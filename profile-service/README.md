
# ğŸ¤– RAG-Based Technical Document Assistant

A secure, local Retrieval-Augmented Generation (RAG) app that helps you analyze and extract insights from structured technical documents (like Use Case Specifications).

---

## ğŸš€ Features

- ğŸ“¤ Upload technical PDFs (e.g., Use Case Specs)
- ğŸ“š Embed & store vectors with ChromaDB
- ğŸ§  Query using a local LLM (via Ollama)
- ğŸ† Re-rank results using sentence-transformer cross-encoders
- ğŸ“ Extract BRs, AFs, XFs, and system message logic from main flows
- ğŸ”’ Fully local/offline setup â€“ no data ever leaves your system

---

## ğŸ“¦ Prerequisites

Before running the project, make sure the following are installed:

### ğŸ› ï¸ System Requirements

- Python 3.11
- [Ollama](https://ollama.com/) (for running local LLMs)

> ğŸ§ª Install Ollama:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

> After installation, start Ollama manually:
```bash
ollama serve
```

Ollama will run a local server on `http://localhost:11434`.

---

## ğŸ¤– Ollama Model Setup

This project requires the following models to be pulled via Ollama:

```bash
ollama pull llama3.2:3b
ollama pull nomic-embed-text:latest
```

These are used for:
- `llama3.2:3b` â†’ main reasoning LLM
- `nomic-embed-text:latest` â†’ embedding function for ChromaDB

Make sure `ollama serve` is running before starting the app.

---

## âš™ï¸ Setup

All project tasks are defined in the `Makefile`.

### ğŸ”¹ 1. Create Virtual Environment & Install Dependencies

```bash
make setup
```

This will:
- Create a `.venv` virtual environment
- Install all Python dependencies inside it

> Activate your environment:
```bash
source .venv/bin/activate
```

---

## ğŸš€ Run the App

```bash
make run
```

Then open your browser and visit:  
ğŸ“ `http://localhost:8501`

---

## ğŸ§¾ Use Case Flow

1. **Upload**: Drop in a use case PDF
2. **Process**: Text is chunked & embedded to ChromaDB
3. **Query**: Ask questions using natural language
4. **Re-rank**: Top results are ranked using CrossEncoder
5. **LLM**: Final answer generated using selected local LLM

---

## ğŸ“‹ Custom Prompt Logic

The assistant will:
- Extract document name from "Use Case Specification" header (e.g., `BNYM_ABC_001`)
- Identify referenced/extension documents in the "Brief Description"
- Answer using only the provided context â€” no external knowledge

You can modify the `system_prompt` in `app.py` to change behavior.

---

## ğŸ“ Project Structure

```bash
ProjectAI/
â”œâ”€â”€ app.py                 # Streamlit app with RAG pipeline
â”œâ”€â”€ Makefile               # All project tasks
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ demo-rag-chroma/       # ChromaDB persistent store
â””â”€â”€ README.md              # This file
```

---

## ğŸ›¡ï¸ Security

âœ… 100% local  
âœ… Offline document parsing  
âœ… Secure embedding and querying  

---

## ğŸ“¬ Contribute / Extend

This app is modular. You can easily extend it to:
- Support DOCX or HTML documents
- Compare across document layers (product â†’ sub-product â†’ bank)
- Output auto-formatted documentation

PRs welcome!

---

### Built for internal document reasoning, legacy analysis, and secure AI workflows.
